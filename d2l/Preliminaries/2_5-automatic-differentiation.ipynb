{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/101_learning_area/blob/area/d2l/Preliminaries/2_5-automatic-differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "**Source**: Aston Zhang et al. 2023. [Dive into deep learning](https://d2l.ai/)."
      ],
      "metadata": {
        "id": "NaevHP91SBLX"
      },
      "id": "NaevHP91SBLX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All modern deep learning frameworks take the complex calculus work off our plates by offering *automatic differentiation* (often shortened to *autograd*)."
      ],
      "metadata": {
        "id": "DfWmT7IWS2vj"
      },
      "id": "DfWmT7IWS2vj"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "wWZdUWvcTzyt"
      },
      "id": "wWZdUWvcTzyt",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple function\n",
        "Let's assume that we are interested in differentiating the function $y=2x^Tx$ with respect to the column vector $x$."
      ],
      "metadata": {
        "id": "1yjqrSJhT1eC"
      },
      "id": "1yjqrSJhT1eC"
    },
    {
      "cell_type": "code",
      "source": [
        "# assign x an initial value\n",
        "x = torch.arange(4.0, requires_grad=True)\n",
        "print(x.grad) # the gradient is None by default"
      ],
      "metadata": {
        "id": "mUYiQJ1YT5dd",
        "outputId": "7e61fede-a7be-4e81-b1ae-df927c74ae90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mUYiQJ1YT5dd",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate y\n",
        "y = 2 * torch.dot(x, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "NPCpagGMVNo3",
        "outputId": "46938a4c-e412-4622-de84-0b2adda0e3b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NPCpagGMVNo3",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(28., grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() # take the gradient of y with respect to x\n",
        "x, x.grad # access the gradient"
      ],
      "metadata": {
        "id": "MLkG6QPPV8ua",
        "outputId": "78ef814d-1f2d-4431-ffc4-3e60ac96d6c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MLkG6QPPV8ua",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([ 0.,  4.,  8., 12.]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already know that the gradient of the function $y=2x^Tx$ with respect to $x$ should be $4x$."
      ],
      "metadata": {
        "id": "u6F-PhMhWkCi"
      },
      "id": "u6F-PhMhWkCi"
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad == 4 * x)"
      ],
      "metadata": {
        "id": "h9CQAGDHWeFv",
        "outputId": "c6e82d0e-5378-4287-cf35-bf9e7cfb0df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h9CQAGDHWeFv",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate another function of x and take its gradient.\n",
        "\n",
        "> **Note** that PyTorch does not automatically reset the gradient buffer when we record a new gradient."
      ],
      "metadata": {
        "id": "Of457pmBXmNb"
      },
      "id": "Of457pmBXmNb"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward() # take the gradient\n",
        "x.grad # access the gradient"
      ],
      "metadata": {
        "id": "4oUP6tvaX8oL",
        "outputId": "c0cd7e69-b8e1-4e3b-f836-5d4acd2785e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4oUP6tvaX8oL",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward for non-scalar variables\n",
        "When $y$ is a vector, the most natural representation of the derivative of $y$ with respecto to a vector $x$ is a matrix called the *Jacobian* that contains the partial derivatives of each component of $y$ with respect to each component of $x$."
      ],
      "metadata": {
        "id": "EiLTb6z-ZaCV"
      },
      "id": "EiLTb6z-ZaCV"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.sum().backward()\n",
        "x, x.grad"
      ],
      "metadata": {
        "id": "sSVr0RWucMm3",
        "outputId": "1d021b16-3817-4aeb-cb23-848c86f8469a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sSVr0RWucMm3",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([0., 2., 4., 6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detaching Computation\n",
        "Suppose we have $z = x * y$ and $y = x * x$ but we want to focus on the direct influence of $x$ on $z$ rather than the influence conveyed via $y$. In this case, we can create a new variable $u$ that takes the same value as $y$ but whose provenance (how it was created) has been wiped out.\n",
        "\n",
        "> Detaching a tensor means that it will no longer be part of the gradient computation during backpropagation."
      ],
      "metadata": {
        "id": "TvGg6IaDctfV"
      },
      "id": "TvGg6IaDctfV"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_() # reset the gradient\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "print(x, '\\n', y, '\\n', u, '\\n', x.grad == u)"
      ],
      "metadata": {
        "id": "SyPkHi7heNz9",
        "outputId": "25a4e0c8-2ffc-4bdf-efe3-7ab5f6a344fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SyPkHi7heNz9",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True) \n",
            " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>) \n",
            " tensor([0., 1., 4., 9.]) \n",
            " tensor([True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the gradient of z = x * u will yield the result u, (not 3 * x * x as you might have expected since z = x * x * x)."
      ],
      "metadata": {
        "id": "Fhbn8DjKiGdr"
      },
      "id": "Fhbn8DjKiGdr"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}