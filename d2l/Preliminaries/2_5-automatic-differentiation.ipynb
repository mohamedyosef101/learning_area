{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedyosef101/101_learning_area/blob/area/d2l/Preliminaries/2_5-automatic-differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "**Source**: Aston Zhang et al. 2023. [Dive into deep learning](https://d2l.ai/)."
      ],
      "metadata": {
        "id": "NaevHP91SBLX"
      },
      "id": "NaevHP91SBLX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "All modern deep learning frameworks take the complex calculus work off our plates by offering *automatic differentiation* (often shortened to *autograd*)."
      ],
      "metadata": {
        "id": "DfWmT7IWS2vj"
      },
      "id": "DfWmT7IWS2vj"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "wWZdUWvcTzyt"
      },
      "id": "wWZdUWvcTzyt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple function\n",
        "Let's assume that we are interested in differentiating the function $y=2x^Tx$ with respect to the column vector $x$."
      ],
      "metadata": {
        "id": "1yjqrSJhT1eC"
      },
      "id": "1yjqrSJhT1eC"
    },
    {
      "cell_type": "code",
      "source": [
        "# assign x an initial value\n",
        "x = torch.arange(4.0, requires_grad=True)\n",
        "print(x.grad) # the gradient is None by default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUYiQJ1YT5dd",
        "outputId": "6a45bbff-ecc5-49e8-fc0c-2ab073715175"
      },
      "id": "mUYiQJ1YT5dd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate y\n",
        "y = 2 * torch.dot(x, x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPCpagGMVNo3",
        "outputId": "0ff6ddf5-2a3b-4a0f-b962-6198ae69870f"
      },
      "id": "NPCpagGMVNo3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(28., grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() # take the gradient of y with respect to x\n",
        "x, x.grad # access the gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLkG6QPPV8ua",
        "outputId": "e596e5bc-52bc-43e3-80d7-d7271df2efcd"
      },
      "id": "MLkG6QPPV8ua",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([ 0.,  4.,  8., 12.]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already know that the gradient of the function $y=2x^Tx$ with respect to $x$ should be $4x$."
      ],
      "metadata": {
        "id": "u6F-PhMhWkCi"
      },
      "id": "u6F-PhMhWkCi"
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad == 4 * x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9CQAGDHWeFv",
        "outputId": "45ce4cdf-2028-4f9c-eb21-0ae3de397299"
      },
      "id": "h9CQAGDHWeFv",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate another function of x and take its gradient.\n",
        "\n",
        "> **Note** that PyTorch does not automatically reset the gradient buffer when we record a new gradient."
      ],
      "metadata": {
        "id": "Of457pmBXmNb"
      },
      "id": "Of457pmBXmNb"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward() # take the gradient\n",
        "x.grad # access the gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oUP6tvaX8oL",
        "outputId": "19db3b3e-d26e-43b0-cbf7-1b2070ba9fa6"
      },
      "id": "4oUP6tvaX8oL",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward for non-scalar variables\n",
        "When $y$ is a vector, the most natural representation of the derivative of $y$ with respecto to a vector $x$ is a matrix called the *Jacobian* that contains the partial derivatives of each component of $y$ with respect to each component of $x$."
      ],
      "metadata": {
        "id": "EiLTb6z-ZaCV"
      },
      "id": "EiLTb6z-ZaCV"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.sum().backward()\n",
        "x, x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSVr0RWucMm3",
        "outputId": "80052f55-2192-46d2-dcd3-7bf19c35caa1"
      },
      "id": "sSVr0RWucMm3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([0., 2., 4., 6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detaching Computation\n",
        "Suppose we have $z = x * y$ and $y = x * x$ but we want to focus on the direct influence of $x$ on $z$ rather than the influence conveyed via $y$. In this case, we can create a new variable $u$ that takes the same value as $y$ but whose provenance (how it was created) has been wiped out.\n",
        "\n",
        "> Detaching a tensor means that it will no longer be part of the gradient computation during backpropagation."
      ],
      "metadata": {
        "id": "TvGg6IaDctfV"
      },
      "id": "TvGg6IaDctfV"
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_() # reset the gradient\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "print(x, '\\n', y, '\\n', u, '\\n', x.grad == u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyPkHi7heNz9",
        "outputId": "f8144ab9-679c-4999-bce5-51cb0c36d6ec"
      },
      "id": "SyPkHi7heNz9",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3.], requires_grad=True) \n",
            " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>) \n",
            " tensor([0., 1., 4., 9.]) \n",
            " tensor([True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the gradient of z = x * u will yield the result u, (not 3 * x * x as you might have expected since z = x * x * x)."
      ],
      "metadata": {
        "id": "Fhbn8DjKiGdr"
      },
      "id": "Fhbn8DjKiGdr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients and Python Control Flow\n"
      ],
      "metadata": {
        "id": "G60TvQcEJVT3"
      },
      "id": "G60TvQcEJVT3"
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "  b = a * 2\n",
        "  while b.norm() < 1000:\n",
        "    b = b * 2\n",
        "  if b.sum() > 0:\n",
        "    c = b\n",
        "  else:\n",
        "    c = 100 * b\n",
        "  return c"
      ],
      "metadata": {
        "id": "t0QEG24MOU90"
      },
      "id": "t0QEG24MOU90",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "ZNbfYmuKOp4Z"
      },
      "id": "ZNbfYmuKOp4Z",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d / a"
      ],
      "metadata": {
        "id": "e5mzh9_hO4Dy",
        "outputId": "f4892c41-ae25-420b-9879-80aae80c7032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e5mzh9_hO4Dy",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}